{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis y trabajo con el dataset 20NewsGroups**\n",
        "\n",
        "<p align=\"justify\">En esta parte del proyecto vamos a trabajar con el dataset 20NewsGroups que engloba mensajes de 20 clases distintas de mensajes de noticias."
      ],
      "metadata": {
        "id": "99BkOp86i-_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis del dataset y preprocesado"
      ],
      "metadata": {
        "id": "5b_aipP5i4vP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">Primero vamos a realizar una prueba para analizar el dataset, para ello vamos a empezar cargando un subconjunto de 7 clases en lugar de las 20 originales.\n",
        "\n",
        "<p align=\"justify\">Sobre este subconjunto, vamos a probar a realizar una clasificación sencilla usando un clasificador bayesinano y vamos a enseñar las 20 palabras más importantes a la hora de realizar esta clasificación en el proceso de entrenamiento."
      ],
      "metadata": {
        "id": "gxo-ZA9ajTix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Codificación del texto**\n",
        "<p align=\"justify\">Una parte fundamental del procesamiento de lenguaje natural es la codificación del texto, en nuestro caso vamos a usar TF-IDF (Term Frequency-Inverse Document Frequency). Esta técnica de codificación usa la frecuencia del término (TF) y la inversa de la frecuencia en el documento (Inverse Document Frequency).\n",
        "\n",
        "<p align=\"justify\">Con esta técnica podemos medir la importancia de un término dentro una clase y entre todas, ya que una palabra que aparece repetidamente en una clase será más relevante que una que aparezca muchas veces en todas las clases.\n",
        "\n",
        "<p align=\"justify\">Podemos definir la fórmula de TD-IDF como la siguiente:\n",
        "TF-IDF(t,d,D) = TF(t,d) * IDF(t,D).\n",
        "\n",
        "<p align=\"justify\">Donde TF(t,d) es la frecuencia de un término t en un documento t. IDF(t,D) es la frecuencia inversa de un término t en el conjunto de documentos D. Para calcular IDF(t,D) usamos log(N/n_t) siendo N el número total de documentos que contiene D y n_t el número de documentos que contienen el término t.\n",
        "\n"
      ],
      "metadata": {
        "id": "mSDz-kcWInHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuMiD51HY__G",
        "outputId": "53acb415-6836-4d31-fbe6-10f80440b39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (1.25.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from scikit-learn) (1.25.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: nltk in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from nltk) (2023.3.22)\n",
            "Requirement already satisfied: click in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\34653\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Función auxiliar para ver las n palabras más importantes a la hora de clasificar\n",
        "def print_n_most_relevant_words(n, classifier, vectorizer, data):\n",
        "  probabilities = np.exp(classifier.feature_log_prob_)\n",
        "  probabilities = np.argsort(-probabilities)\n",
        "  feature_names = vectorizer.get_feature_names_out()\n",
        "  for i, target_name in enumerate(data.target_names):\n",
        "    n_words_idx = probabilities[i][:n]\n",
        "    words = [feature_names[idx] for idx in n_words_idx]\n",
        "    print('\\n' + target_name + ': ' + ' '.join(words))\n"
      ],
      "metadata": {
        "id": "rJimUvqzjgu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos el dataset, el vectorizador y el clasificador\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "# Elegimos las 8 categorías que vamos a cargar\n",
        "categories = [\n",
        "  'alt.atheism',\n",
        "  'comp.graphics',\n",
        "  'comp.os.ms-windows.misc',\n",
        "  'comp.sys.ibm.pc.hardware',\n",
        "  'comp.sys.mac.hardware',\n",
        "  'comp.windows.x',\n",
        "  'sci.crypt',\n",
        "  'sci.electronics']\n",
        "# Cargamos el dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "# Vectorizamos el texto\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(vectors_train, newsgroups_train.target)\n",
        "print_n_most_relevant_words(20, clf, vectorizer, newsgroups_train)\n",
        "# Realizamos la predicción con los datos de test\n",
        "pred = clf.predict(vectors_test)\n",
        "print('\\n' + classification_report(newsgroups_test.target, pred, target_names=newsgroups_test.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB28ohlMi3zY",
        "outputId": "b43fd6a1-f8d6-41e8-94a9-d0c8380611ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "alt.atheism: the of to that is in you and it edu god keith not are be he caltech was as we\n",
            "\n",
            "comp.graphics: the to of and graphics is in for it edu from that you on this image have be or university\n",
            "\n",
            "comp.os.ms-windows.misc: the windows to it edu and is of in for you that file have dos with from on driver this\n",
            "\n",
            "comp.sys.ibm.pc.hardware: the to and scsi it of is drive for edu with in ide on that card com have bus my\n",
            "\n",
            "comp.sys.mac.hardware: the to edu of and mac is apple it in for that with on you from have my this drive\n",
            "\n",
            "comp.windows.x: the to window of and is in com motif it for edu this mit on server that you from with\n",
            "\n",
            "sci.crypt: the to of and key is that clipper in it encryption be chip com they this for you as government\n",
            "\n",
            "sci.electronics: the to of and is in edu you it for that on are com from have be this with if\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.99      0.91      0.95       319\n",
            "           comp.graphics       0.82      0.67      0.74       389\n",
            " comp.os.ms-windows.misc       0.81      0.65      0.72       394\n",
            "comp.sys.ibm.pc.hardware       0.70      0.78      0.74       392\n",
            "   comp.sys.mac.hardware       0.88      0.80      0.84       385\n",
            "          comp.windows.x       0.90      0.74      0.82       395\n",
            "               sci.crypt       0.54      0.98      0.70       396\n",
            "         sci.electronics       0.85      0.65      0.74       393\n",
            "\n",
            "                accuracy                           0.77      3063\n",
            "               macro avg       0.81      0.77      0.78      3063\n",
            "            weighted avg       0.81      0.77      0.78      3063\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<p align=\"justify\">Como se puede apreciar en la ejecución de la celda de código anterior, entras las palabras más relevantes a la hora de clasificar, hay palabras muy genéricas como pueden ser artículos (the), preposiciones (to, of, in, from, on, with), demostrativos (this, that), pronombres personales(You, We), conjunciones (and). También nos encontramos palabras como edu que dan información no sobre la temático del texto, sino del remitente (haciendo referencia a su pertenencia a una universidad)."
      ],
      "metadata": {
        "id": "C4PN2pIBi4-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">El resultado anterior justifica realizar un proprocesado a nuestros datos, para ello vamos a eliminar las cabeceras, los pies de página y las citas. También vamos a eliminar las palabras que no aportan información que hemos citado anteriormente.\n",
        "\n",
        "<p align=\"justify\">Otro preprocesado que vamos a realizar sobre el dataset es un generalizado de palabras, así, podremos reducir el tamaño del vocabulario propio de cada clase teniendo en cuenta la palabra base. Un ejemplo de esto sería de la palabra 'having' obtener la palabra 'have'.\n",
        "\n",
        "<p align=\"justify\">Aplicando este preprocesadolos resultados que obtengamos serán más cercanos a la realidad, los actuales tienen ruido debido a la presencia de estas palabras."
      ],
      "metadata": {
        "id": "FJtyqbrxhD9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función auxiliar para realizar la generalización\n",
        "def lemmatize(tokens, lemmatizer):\n",
        "    lemmatized_tokens = []\n",
        "    for token in tokens:\n",
        "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "AZKQLlS75gZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos el lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import string\n",
        "# Cargamos una lista de signos de puntuación para eliminarlos al preprocesar\n",
        "punctuations = string.punctuation + '...' + '--' + '``' + '\\'' + ''\n",
        "# Función para realizar el tokenizado desde el texto (usa la función auxiliar de stemming)\n",
        "def tokenize(text, lemmatizer=WordNetLemmatizer()):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [i for i in tokens if i not in punctuations]\n",
        "    lemmatized_tokens = lemmatize(tokens, lemmatizer)\n",
        "    #stemmed_tokens = [i for i in stemmed_tokens if i not in punctuations]\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGU_W3-Z55N_",
        "outputId": "1f06bb0a-89de-4fda-f4b9-d1bde0b6eba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\34653\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\34653\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el dataset sin cabeceras, pies de página ni citas\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),categories=categories)\n",
        "# Tokenizamos y clasificamos el texto\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
        "vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(vectors_train, newsgroups_train.target)\n",
        "print_n_most_relevant_words(20, clf, vectorizer, newsgroups_train)\n",
        "# Realizamos la predicción con los datos de test\n",
        "pred = clf.predict(vectors_test)\n",
        "print('\\n' + classification_report(newsgroups_test.target, pred, target_names=newsgroups_test.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFebEIL5awMg",
        "outputId": "886c584f-1ad9-442b-a733-107b2a8ec38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "C:\\Users\\34653\\anaconda3\\envs\\colab\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "alt.atheism: '' n't god wa 's people think religion atheist say just doe thing belief moral bible did argument ha said\n",
            "\n",
            "comp.graphics: file image graphic program thanks n't know format '' 's doe color looking need point 'm wa 3d help package\n",
            "\n",
            "comp.os.ms-windows.misc: window file driver problem '' program 's n't use font 3.1 card know thanks 'ax doe wa using version like\n",
            "\n",
            "comp.sys.ibm.pc.hardware: drive card controller ide n't scsi bus disk monitor ha problem doe thanks 's pc '' port know board wa\n",
            "\n",
            "comp.sys.mac.hardware: mac apple drive problem 's n't doe monitor card '' ha know thanks simms work just quadra use 'm centris\n",
            "\n",
            "comp.windows.x: x window server widget '' motif application thanks 's use program file n't doe display problem using ha know client\n",
            "\n",
            "sci.crypt: key '' chip encryption n't clipper government 's phone people nsa ha wa just use escrow algorithm doe security law\n",
            "\n",
            "sci.electronics: 's n't wa '' circuit power like know use doe line ground good chip used voltage ha current amp radio\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.96      0.66      0.79       319\n",
            "           comp.graphics       0.74      0.67      0.70       389\n",
            " comp.os.ms-windows.misc       0.70      0.54      0.61       394\n",
            "comp.sys.ibm.pc.hardware       0.64      0.73      0.68       392\n",
            "   comp.sys.mac.hardware       0.76      0.67      0.71       385\n",
            "          comp.windows.x       0.80      0.78      0.79       395\n",
            "               sci.crypt       0.49      0.92      0.64       396\n",
            "         sci.electronics       0.82      0.58      0.68       393\n",
            "\n",
            "                accuracy                           0.70      3063\n",
            "               macro avg       0.74      0.69      0.70      3063\n",
            "            weighted avg       0.73      0.70      0.70      3063\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">Podemos observar como los resultados han empeorado con respecto a la clasificación realizada sin preprocesar. Pero, si nos fijamos en las palabras más significativas a la hora de clasificar, ahora sí que podemos ver como estas se ajustan a la temática de la que se está hablando, por lo que podemos concluir que los resultados actuales reflejan mejor la realidad que los anteriores."
      ],
      "metadata": {
        "id": "LkD8FWI5okyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparación de clasificadores"
      ],
      "metadata": {
        "id": "m89bExJ_i5Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\"> Una vez hemos realizado el análisis del dataset y hemos justificado porqué es necesario realizar un preprocesamiento de los datos, procedemos a realizar una comparativa de distintos clasificadores para valorar los rendimientos obtenidos. Para ello utilizaremos el dataset completo de 20 categorías. Para realizar una comparación adecuado usaremos validación cruzada estratificada para así mantener el porcentaje de presencia de cada clase en los distintos conjuntos que se crean a la hora de realizar la validación."
      ],
      "metadata": {
        "id": "2f8Q0O8QxJwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una función para imprimir por pantalla el informe de clasificación medio de todos los informes al usar Kfold CrossValidation\n",
        "def print_mean_classification_report(classification_reports):\n",
        "  n_folds = len(classification_reports)\n",
        "  total_report = {}\n",
        "\n",
        "  # Sumamos para cada métrica\n",
        "  for report in classification_reports:\n",
        "      for metric, scores in report.items():\n",
        "\n",
        "        if metric not in total_report:\n",
        "          if metric != 'accuracy':\n",
        "            total_report[metric] = scores\n",
        "\n",
        "        else:\n",
        "          if metric not in ['accuracy']:\n",
        "            total_report[metric] = {k: total_report[metric][k] + scores[k] for k in scores}\n",
        "\n",
        "  # Dividimos entre el número de folds de validación\n",
        "  mean_report = {metric: {k: v/n_folds for k, v in scores.items()} for metric, scores in total_report.items()}\n",
        "\n",
        "  # Damos formato\n",
        "  fmt_string = '{:<25} {:<25} {:<25} {:<25} {:<25}'\n",
        "\n",
        "  # Mostramos por pantalla las métricas\n",
        "  print(fmt_string.format('', 'precision', 'recall', 'f1-score', 'support'))\n",
        "\n",
        "  # Mostramos el informe de clasificiación\n",
        "  for metric, scores in mean_report.items():\n",
        "      precision = scores['precision']\n",
        "      recall = scores['recall']\n",
        "      f1_score = scores['f1-score']\n",
        "      support = scores['support']\n",
        "      print(fmt_string.format(metric, precision, recall, f1_score, support))"
      ],
      "metadata": {
        "id": "ejYErib6nIdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasificador de Bayes Ingenuo**\n",
        "\n",
        "<p align=\"justify\">El primer clasificador que vamos a usar es el que hemos estado trabajando para presentar el dataset y la justificación del preprocesado. Se trata del clasificador de Bayes ingenuo. Es un clasificador probabilístico cuyo fundamento recae en la regla de Bayes sobre la probabilidad condicionada. Recibe el adjetivo de ingenuo ya que el modelo asume que la independencia de las características usadas en la clasificación."
      ],
      "metadata": {
        "id": "n_lL-sxseOTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignoramos los warnings de la categoría UserWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Importamos Validación Cruzada Estratificada\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "# Nos quedamos con los nombres de las clases para el informe de clasificación\n",
        "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "labels_name = dataset.target_names\n",
        "\n",
        "# Cargamos el dataset completo\n",
        "data, labels = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "data, labels = np.asarray(data), np.asarray(labels)\n",
        "\n",
        "# Creamos una lista para guardar los distintos classification reports y las accuracies\n",
        "classification_reports = []\n",
        "accuracies = []\n",
        "\n",
        "# Obtenemos los conjuntos de entrenamiento y test para cada ejecución\n",
        "for i, (train_index, test_index) in enumerate(skfold.split(data, labels)):\n",
        "  data_train, data_test = data[train_index], data[test_index]\n",
        "  labels_train, labels_test = labels[train_index], labels[test_index]\n",
        "\n",
        "  # Realizamos el preprocesamiento de cada conjunto\n",
        "  vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
        "  vectors_train = vectorizer.fit_transform(data_train)\n",
        "  vectors_test = vectorizer.transform(data_test)\n",
        "\n",
        "  # Realizamos el entrenamiento\n",
        "  clf = MultinomialNB()\n",
        "  clf.fit(vectors_train, labels_train)\n",
        "\n",
        "  # Realizamos la predicción con los datos de test\n",
        "  accuracy = clf.score(vectors_test, labels_test)\n",
        "  accuracies.append(accuracy)\n",
        "  pred = clf.predict(vectors_test)\n",
        "  report = classification_report(labels_test, pred, target_names=labels_name, output_dict=True, zero_division=0)\n",
        "  classification_reports.append(report)\n",
        "\n",
        "print_mean_classification_report(classification_reports)\n",
        "print(\"Accuracy: \" , np.mean(np.array(accuracies)))"
      ],
      "metadata": {
        "id": "r16tjUBa9w3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "981a8e0e-2d97-4e4b-d6a8-49ae2eb68304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision                 recall                    f1-score                  support                  \n",
            "alt.atheism               0.8219850490011782        0.30802215189873416       0.44591151391435224       79.9                     \n",
            "comp.graphics             0.7095113692718791        0.6845150431306543        0.6957521545839586        97.3                     \n",
            "comp.os.ms-windows.misc   0.7374916216458959        0.664904143475572         0.6984956435839692        98.5                     \n",
            "comp.sys.ibm.pc.hardware  0.646102483377013         0.7688414759843331        0.7017831491701538        98.2                     \n",
            "comp.sys.mac.hardware     0.8519460339211996        0.7051224226804125        0.7704828578316578        96.3                     \n",
            "comp.windows.x            0.8144542830147387        0.8431148216862502        0.827806738359415         98.8                     \n",
            "misc.forsale              0.835585846458452         0.7406164527666736        0.7837513706739458        97.5                     \n",
            "rec.autos                 0.8510865860234584        0.7525252525252525        0.7978570408099573        99.0                     \n",
            "rec.motorcycles           0.8698826939420317        0.7580505050505051        0.8095685249498974        99.6                     \n",
            "rec.sport.baseball        0.9038453660259946        0.8239090909090908        0.8617002874588019        99.4                     \n",
            "rec.sport.hockey          0.5857698798247313        0.9289191919191918        0.7177133121124013        99.9                     \n",
            "sci.crypt                 0.6515803821896103        0.8476969696969696        0.7357543670527482        99.1                     \n",
            "sci.electronics           0.8143691492790405        0.6494227994227995        0.7220573515368296        98.4                     \n",
            "sci.med                   0.8898544207261475        0.8292929292929292        0.8578074259013155        99.0                     \n",
            "sci.space                 0.8479194333657217        0.77723149866007          0.810454150164885         98.7                     \n",
            "soc.religion.christian    0.4144223115425514        0.9377979797979797        0.5746842021523502        99.7                     \n",
            "talk.politics.guns        0.6468975460706227        0.779120879120879         0.7065404841056155        91.0                     \n",
            "talk.politics.mideast     0.7961968028453169        0.8436170212765959        0.8185322317952352        94.0                     \n",
            "talk.politics.misc        0.8996163080373607        0.39224109224109227       0.5442095614655776        77.5                     \n",
            "talk.religion.misc        0.5833333333333334        0.019098822324628777      0.03687791375291376       62.8                     \n",
            "macro avg                 0.758592544994814         0.7027030271930308        0.695887014068799         1884.6                   \n",
            "weighted avg              0.7598183216098878        0.7228593995505923        0.7122437187731518        1884.6                   \n",
            "Accuracy:  0.7228593995505921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN**\n",
        "\n",
        "<p align=\"justify\">Otro clasificador que vamos a usar es KNN, un algoritmo de clasificación sencillo dónde se utiliza la distancia del dato a clasificar con respecto a los otros datos clasificados, teniendo en cuenta los K vecinos más cercanos. De esos vecinos, se obtiene como predicción la clase a la que pertenezcan la mayor proporción de vecinos."
      ],
      "metadata": {
        "id": "MoH-9RBpnq2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignoramos los warnings de la categoría UserWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Importamos el clasificador\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Importamos Validación Cruzada Estratificada\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "# Nos quedamos con los nombres de las clases para el informe de clasificación\n",
        "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "labels_name = dataset.target_names\n",
        "\n",
        "# Cargamos el dataset completo\n",
        "data, labels = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "data, labels = np.asarray(data), np.asarray(labels)\n",
        "\n",
        "# Creamos una lista para guardar los distintos classification reports y las accuracies\n",
        "classification_reports = []\n",
        "accuracies = []\n",
        "\n",
        "# Obtenemos los conjuntos de entrenamiento y test para cada ejecución\n",
        "for i, (train_index, test_index) in enumerate(skfold.split(data, labels)):\n",
        "  data_train, data_test = data[train_index], data[test_index]\n",
        "  labels_train, labels_test = labels[train_index], labels[test_index]\n",
        "\n",
        "  # Realizamos el preprocesamiento de cada conjunto\n",
        "  vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
        "  vectors_train = vectorizer.fit_transform(data_train)\n",
        "  vectors_test = vectorizer.transform(data_test)\n",
        "\n",
        "  # Creamos el clasificador\n",
        "  knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "  # Realizamos el entrenamiento\n",
        "  knn.fit(vectors_train,labels_train)\n",
        "\n",
        "  # Realizamos la predicción con los datos de test\n",
        "  accuracy = knn.score(vectors_test, labels_test)\n",
        "  accuracies.append(accuracy)\n",
        "  pred = knn.predict(vectors_test)\n",
        "  report = classification_report(labels_test, pred, target_names=labels_name, output_dict=True, zero_division=0)\n",
        "  classification_reports.append(report)\n",
        "\n",
        "print_mean_classification_report(classification_reports)\n",
        "print(\"Accuracy: \" , np.mean(np.array(accuracies)))"
      ],
      "metadata": {
        "id": "kAifMeSbnqSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81706d0d-5546-41ba-e155-41c8741ead14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision                 recall                    f1-score                  support                  \n",
            "alt.atheism               0.8396621580174213        0.13520569620253164       0.23123711712079986       79.9                     \n",
            "comp.graphics             0.8421158579982109        0.10793183252682517       0.1906077375500038        97.3                     \n",
            "comp.os.ms-windows.misc   0.5535793549026866        0.299443413729128         0.17967997750795908       98.5                     \n",
            "comp.sys.ibm.pc.hardware  0.36069361082731793       0.12525252525252525       0.14696881629854897       98.2                     \n",
            "comp.sys.mac.hardware     0.8066666666666666        0.03217353951890034       0.061670873449890015      96.3                     \n",
            "comp.windows.x            0.9052777777777777        0.06985157699443414       0.12944579477289758       98.8                     \n",
            "misc.forsale              0.9214285714285715        0.055343993267410053      0.10374823212254725       97.5                     \n",
            "rec.autos                 0.5833333333333333        0.01919191919191919       0.03702979938638771       99.0                     \n",
            "rec.motorcycles           0.9833333333333334        0.05321212121212121       0.09997993561032201       99.6                     \n",
            "rec.sport.baseball        0.8300000000000001        0.0362020202020202        0.06900616094930934       99.4                     \n",
            "rec.sport.hockey          0.894802532348972         0.15314141414141413       0.2604789074419834        99.9                     \n",
            "sci.crypt                 1.0                       0.04240404040404041       0.08025192803701832       99.1                     \n",
            "sci.electronics           0.8512626262626263        0.05684394970109256       0.105673070957599         98.4                     \n",
            "sci.med                   0.9                       0.06767676767676768       0.12439477742737264       99.0                     \n",
            "sci.space                 0.8949999999999999        0.032446918161203875      0.06216236875631507       98.7                     \n",
            "soc.religion.christian    0.6657142857142857        0.03303030303030303       0.061642673690379825      99.7                     \n",
            "talk.politics.guns        0.805                     0.023076923076923078      0.04457558319420646       91.0                     \n",
            "talk.politics.mideast     0.9613558776167471        0.22127659574468087       0.3585075394140901        94.0                     \n",
            "talk.politics.misc        0.9149999999999998        0.051648351648351645      0.09708593957712383       77.5                     \n",
            "talk.religion.misc        0.15635774337064548       0.7919610855094725        0.08522648608794453       62.8                     \n",
            "macro avg                 0.7835291864799299        0.12036574935960324       0.12646868596763494       1884.6                   \n",
            "weighted avg              0.7931755664800272        0.10835160812538365       0.12633984164690631       1884.6                   \n",
            "Accuracy:  0.10835160812538366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regresión Logística Multinomial**\n",
        "\n",
        "<p align=\"justify\">También vamos a comparar usando la regresión logística multinomial, modelo que calcula la probabilidad de pertenecer a una clase calculando la relación entre una variable dependiente y varias variables independientes. Se utilizan varias funciones logísticas para calcular la probabilidad de pertenecer a cada una de las clases de la variable dependiente, en función de las variables independientes.\n",
        "\n",
        "Usaremos saga como solver de la regresión logística debido al tamaño del dataset y su dimensionalidad."
      ],
      "metadata": {
        "id": "jcqjhwFUA1Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignoramos los warnings de la categoría UserWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Importamos el clasificador\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Importamos Validación Cruzada Estratificada\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "# Nos quedamos con los nombres de las clases para el informe de clasificación\n",
        "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "labels_name = dataset.target_names\n",
        "\n",
        "# Cargamos el dataset completo\n",
        "data, labels = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "data, labels = np.asarray(data), np.asarray(labels)\n",
        "\n",
        "# Creamos una lista para guardar los distintos classification reports y las accuracies\n",
        "classification_reports = []\n",
        "accuracies = []\n",
        "\n",
        "# Obtenemos los conjuntos de entrenamiento y test para cada ejecución\n",
        "for i, (train_index, test_index) in enumerate(skfold.split(data, labels)):\n",
        "  data_train, data_test = data[train_index], data[test_index]\n",
        "  labels_train, labels_test = labels[train_index], labels[test_index]\n",
        "\n",
        "  # Realizamos el preprocesamiento de cada conjunto\n",
        "  vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
        "  vectors_train = vectorizer.fit_transform(data_train)\n",
        "  vectors_test = vectorizer.transform(data_test)\n",
        "\n",
        "  # Creamos el clasificador\n",
        "  lr_multinomial = LogisticRegression(multi_class='multinomial', solver='saga')\n",
        "\n",
        "  # Realizamos el entrenamiento\n",
        "  lr_multinomial.fit(vectors_train, labels_train)\n",
        "\n",
        "  # Realizamos la predicción con los datos de test\n",
        "  accuracy = lr_multinomial.score(vectors_test, labels_test)\n",
        "  accuracies.append(accuracy)\n",
        "  pred = lr_multinomial.predict(vectors_test)\n",
        "  report = classification_report(labels_test, pred, target_names=labels_name, output_dict=True, zero_division=0)\n",
        "  classification_reports.append(report)\n",
        "\n",
        "print_mean_classification_report(classification_reports)\n",
        "print(\"Accuracy: \" , np.mean(np.array(accuracies)))"
      ],
      "metadata": {
        "id": "v_H4Ba-zFgAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c7a57c-ca59-4742-ff36-292d6810a865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision                 recall                    f1-score                  support                  \n",
            "alt.atheism               0.6440589107724657        0.603386075949367         0.6202849922120348        79.9                     \n",
            "comp.graphics             0.710792593408503         0.7102566799915844        0.7095084979654057        97.3                     \n",
            "comp.os.ms-windows.misc   0.7056111564633679        0.682106782106782         0.6926004602107583        98.5                     \n",
            "comp.sys.ibm.pc.hardware  0.7055748783490752        0.701669758812616         0.7030914506426363        98.2                     \n",
            "comp.sys.mac.hardware     0.8133089290024124        0.7113187285223368        0.7571022139755749        96.3                     \n",
            "comp.windows.x            0.8128199976692887        0.8006493506493506        0.8062527743507211        98.8                     \n",
            "misc.forsale              0.7755881559914389        0.7764569745423942        0.7747497404390257        97.5                     \n",
            "rec.autos                 0.7213116589995111        0.7585858585858587        0.7322835986528113        99.0                     \n",
            "rec.motorcycles           0.5510923593086307        0.8102424242424242        0.6495246881237636        99.6                     \n",
            "rec.sport.baseball        0.8345333943214873        0.832939393939394         0.8330070836015457        99.4                     \n",
            "rec.sport.hockey          0.9177146280474086        0.8668080808080809        0.891361878774093         99.9                     \n",
            "sci.crypt                 0.8623350298454175        0.763929292929293         0.8083743194629813        99.1                     \n",
            "sci.electronics           0.6678208159524786        0.7245928674500103        0.6941650261389651        98.4                     \n",
            "sci.med                   0.8176596413859503        0.8484848484848486        0.8319910316033956        99.0                     \n",
            "sci.space                 0.8085204084407197        0.7843022057307772        0.7948052697225858        98.7                     \n",
            "soc.religion.christian    0.7196119505193159        0.8003737373737374        0.7567393138250876        99.7                     \n",
            "talk.politics.guns        0.6978269094373004        0.7285714285714286        0.7123080009481295        91.0                     \n",
            "talk.politics.mideast     0.8729001718261514        0.8042553191489361        0.8359962442595691        94.0                     \n",
            "talk.politics.misc        0.6841632671757185        0.6245421245421247        0.6520509105095613        77.5                     \n",
            "talk.religion.misc        0.6149992392399681        0.28509984639016894       0.387558923349398         62.8                     \n",
            "macro avg                 0.7469122048078305        0.7309285889385755        0.7321878209384022        1884.6                   \n",
            "weighted avg              0.751061803358297         0.7419607810009744        0.7407104488172452        1884.6                   \n",
            "Accuracy:  0.7419607810009743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**\n",
        "\n",
        "<p align=\"justify\">Otro clasificador que vamos a usar es SVM (Support Vector Machines) que clasifica obteniendo hiperplanos que se usan para separar los datos en las distintas clases. Vamos a usar como kernel RBF (Radial Basis Function) ya que nos permite tomar decisiones no lineales en espacios de características de alta dimensionalidad como es el dataset 20newsgroups."
      ],
      "metadata": {
        "id": "VZwI85rxKdE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignoramos los warnings de la categoría UserWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Importamos el clasificador\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Importamos Validación Cruzada Estratificada\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "# Nos quedamos con los nombres de las clases para el informe de clasificación\n",
        "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "labels_name = dataset.target_names\n",
        "\n",
        "# Cargamos el dataset completo\n",
        "data, labels = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "data, labels = np.asarray(data), np.asarray(labels)\n",
        "\n",
        "# Creamos una lista para guardar los distintos classification reports y las accuracies\n",
        "classification_reports = []\n",
        "accuracies = []\n",
        "\n",
        "# Obtenemos los conjuntos de entrenamiento y test para cada ejecución\n",
        "for i, (train_index, test_index) in enumerate(skfold.split(data, labels)):\n",
        "  data_train, data_test = data[train_index], data[test_index]\n",
        "  labels_train, labels_test = labels[train_index], labels[test_index]\n",
        "\n",
        "  # Realizamos el preprocesamiento de cada conjunto\n",
        "  vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
        "  vectors_train = vectorizer.fit_transform(data_train)\n",
        "  vectors_test = vectorizer.transform(data_test)\n",
        "\n",
        "  # Creamos el clasificador\n",
        "  svm = SVC(kernel='rbf')\n",
        "\n",
        "  # Realizamos el entrenamiento\n",
        "  svm.fit(vectors_train, labels_train)\n",
        "\n",
        "  # Realizamos la predicción con los datos de test\n",
        "  accuracy = svm.score(vectors_test, labels_test)\n",
        "  accuracies.append(accuracy)\n",
        "  pred = svm.predict(vectors_test)\n",
        "  report = classification_report(labels_test, pred, target_names=labels_name, output_dict=True, zero_division=0)\n",
        "  classification_reports.append(report)\n",
        "\n",
        "print_mean_classification_report(classification_reports)\n",
        "print(\"Accuracy: \" , np.mean(np.array(accuracies)))\n"
      ],
      "metadata": {
        "id": "EZu7rlxE-JNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447749bb-969a-420a-9e92-bb724e1f336b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision                 recall                    f1-score                  support                  \n",
            "alt.atheism               0.6302352301937099        0.6346518987341773        0.6299055283722305        79.9                     \n",
            "comp.graphics             0.6685278069533926        0.7338523038081212        0.6985874382967128        97.3                     \n",
            "comp.os.ms-windows.misc   0.7278168131088877        0.6728715728715728        0.6983612140998225        98.5                     \n",
            "comp.sys.ibm.pc.hardware  0.7251617672421224        0.7138631210059782        0.7189145507542806        98.2                     \n",
            "comp.sys.mac.hardware     0.8407955689341277        0.6905605670103093        0.7570760743700322        96.3                     \n",
            "comp.windows.x            0.8495400808228372        0.7904968047825192        0.818283219295872         98.8                     \n",
            "misc.forsale              0.8058419173676115        0.7713023353671364        0.7867051315977518        97.5                     \n",
            "rec.autos                 0.5342576278660054        0.7898989898989898        0.6361069661577091        99.0                     \n",
            "rec.motorcycles           0.6677139492339343        0.8122525252525253        0.7321808121078387        99.6                     \n",
            "rec.sport.baseball        0.8733407323019964        0.8258989898989899        0.8484049590068828        99.4                     \n",
            "rec.sport.hockey          0.95549300658974          0.8367878787878787        0.8920416647882108        99.9                     \n",
            "sci.crypt                 0.8952161288460087        0.7336767676767678        0.8054009947470915        99.1                     \n",
            "sci.electronics           0.5750750004715033        0.7621727478870338        0.6547609307715992        98.4                     \n",
            "sci.med                   0.8131628270746083        0.8414141414141415        0.8265569554958482        99.0                     \n",
            "sci.space                 0.7625336658707228        0.7913832199546487        0.7752217539541614        98.7                     \n",
            "soc.religion.christian    0.7410574755066913        0.8074141414141414        0.7721139665559053        99.7                     \n",
            "talk.politics.guns        0.6916796736286648        0.743956043956044         0.7161503941958304        91.0                     \n",
            "talk.politics.mideast     0.917167486908658         0.7861702127659573        0.8453680232215778        94.0                     \n",
            "talk.politics.misc        0.7144999991212252        0.6219613719613719        0.6637760172655605        77.5                     \n",
            "talk.religion.misc        0.7279152912305087        0.2484639016897081        0.36916965096537485       62.8                     \n",
            "macro avg                 0.7558516024636479        0.7304524768069006        0.7322543123010147        1884.6                   \n",
            "weighted avg              0.7580499817034749        0.7418550181058418        0.7409081146771379        1884.6                   \n",
            "Accuracy:  0.7418550181058418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**\n",
        "<p align=\"justify\">Otro clasificador con el que vamos a comparar es RandomForest, algoritmo en el que construímos un número determinado de arboles de decisión, cada uno de ellos se entra con una muestra aleatoria del conjunto de datos de manera independiente y se usa para clasificar un dato. La clasificación que usa el algoritmo es la que hacen la mayoría de los árboles."
      ],
      "metadata": {
        "id": "rg5mG2uSEOUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignoramos los warnings de la categoría UserWarning\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Importamos el clasificador\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Importamos Validación Cruzada Estratificada\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "\n",
        "# Nos quedamos con los nombres de las clases para el informe de clasificación\n",
        "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "labels_name = dataset.target_names\n",
        "\n",
        "# Cargamos el dataset completo\n",
        "data, labels = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
        "data, labels = np.asarray(data), np.asarray(labels)\n",
        "\n",
        "# Creamos una lista para guardar los distintos classification reports y las accuracies\n",
        "classification_reports = []\n",
        "accuracies = []\n",
        "\n",
        "# Obtenemos los conjuntos de entrenamiento y test para cada ejecución\n",
        "for i, (train_index, test_index) in enumerate(skfold.split(data, labels)):\n",
        "  data_train, data_test = data[train_index], data[test_index]\n",
        "  labels_train, labels_test = labels[train_index], labels[test_index]\n",
        "\n",
        "  # Realizamos el preprocesamiento de cada conjunto\n",
        "  vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
        "  vectors_train = vectorizer.fit_transform(data_train)\n",
        "  vectors_test = vectorizer.transform(data_test)\n",
        "\n",
        "  # Creamos el clasificador\n",
        "  rf = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "  # Realizamos el entrenamiento\n",
        "  rf.fit(vectors_train, labels_train)\n",
        "\n",
        "  # Realizamos la predicción con los datos de test\n",
        "  accuracy = rf.score(vectors_test, labels_test)\n",
        "  accuracies.append(accuracy)\n",
        "  pred = rf.predict(vectors_test)\n",
        "  report = classification_report(labels_test, pred, target_names=labels_name, output_dict=True, zero_division=0)\n",
        "  classification_reports.append(report)\n",
        "\n",
        "print_mean_classification_report(classification_reports)\n",
        "print(\"Accuracy: \" , np.mean(np.array(accuracies)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvqwXE6MEPGq",
        "outputId": "1fd68b1f-5ece-48f1-c77a-0a2339969a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          precision                 recall                    f1-score                  support                  \n",
            "alt.atheism               0.5553893041282139        0.463243670886076         0.5034981566313669        79.9                     \n",
            "comp.graphics             0.6115962658175936        0.6002945508100146        0.6047559919388326        97.3                     \n",
            "comp.os.ms-windows.misc   0.6071755966218941        0.6648938363224077        0.6344507282517873        98.5                     \n",
            "comp.sys.ibm.pc.hardware  0.6119525934493146        0.6334570191713048        0.6211060475759929        98.2                     \n",
            "comp.sys.mac.hardware     0.7219801689676302        0.6584192439862543        0.6877660635540344        96.3                     \n",
            "comp.windows.x            0.7266208234733089        0.7742630385487528        0.7489705516600063        98.8                     \n",
            "misc.forsale              0.7124313664859883        0.7487586787292237        0.7295190647961469        97.5                     \n",
            "rec.autos                 0.46771881530605786       0.7343434343434343        0.5709175696319726        99.0                     \n",
            "rec.motorcycles           0.6960727373136947        0.7269595959595959        0.7100665076813375        99.6                     \n",
            "rec.sport.baseball        0.7128868153869006        0.7685656565656566        0.7390812697017396        99.4                     \n",
            "rec.sport.hockey          0.8021605866635342        0.8558080808080808        0.827764136323875         99.9                     \n",
            "sci.crypt                 0.7835558907306459        0.7538484848484848        0.7674645568339351        99.1                     \n",
            "sci.electronics           0.6420768951168172        0.523294166151309         0.5733083321208675        98.4                     \n",
            "sci.med                   0.7984251618173326        0.7616161616161615        0.7787995401185344        99.0                     \n",
            "sci.space                 0.7815608035012229        0.7195011337868482        0.748079799471795         98.7                     \n",
            "soc.religion.christian    0.6348283872339823        0.8013838383838383        0.7077270468085429        99.7                     \n",
            "talk.politics.guns        0.6241823197048163        0.6791208791208792        0.6494294256914289        91.0                     \n",
            "talk.politics.mideast     0.8228248806013736        0.776595744680851         0.7984408097044048        94.0                     \n",
            "talk.politics.misc        0.6783972583229458        0.41821511821511814       0.5161479914085503        77.5                     \n",
            "talk.religion.misc        0.45642946916433624       0.13059395801331283       0.2005833649413355        62.8                     \n",
            "macro avg                 0.6724133069903802        0.6596588145473803        0.6558938477423242        1884.6                   \n",
            "weighted avg              0.6775284395494533        0.6743599035856888        0.6675158811583312        1884.6                   \n",
            "Accuracy:  0.6743599035856888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\"> De los resultados obtenidos podemos observar que tenemos tres algoritmos que realizan una clasificación con una precisión media de entre el 73-75%. Estos son: Clasificador de Bayes Ingenuo, Regresión Logística Multinomial y SVM.\n",
        "\n"
      ],
      "metadata": {
        "id": "OmlXsr1pxX1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión"
      ],
      "metadata": {
        "id": "uCmoKmOSi5cK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">En este capítulo hemos hecho una introducción al Procesamiento de Lenguaje Natural, para ello hemos hecho un estudio del dataset 20newsgroups. Hemos analizado el comportamiento del dataset con un subconjunto, y una vez que hemos demostrado la necesidad del preprocesamiento hemos realizado una comparación de distintos métodos de clasificación para evaluar su rendimiento en el estudio de este conjunto de datos. En el siguiente capítulo veremos como aplicar esto al problema propio que vamos a desarrollar en este trabajo."
      ],
      "metadata": {
        "id": "lBvEagFN0GCk"
      }
    }
  ]
}